{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d95a5ba215b5c0d3c5562e0f5b2730844d9eca1086482e66ee8573214ced8df7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab.3: Morphology\n",
    "## Introduction to Human Language Technologies\n",
    "### Victor Badenas Crespo\n",
    "\n",
    "***\n",
    "\n",
    "### Statement:\n",
    "\n",
    "- Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "\n",
    "- Compute their similarities by considering lemmas and Jaccard distance.\n",
    "\n",
    "- Compare the results with those in session 2 (document structure) in which words were considered.\n",
    "\n",
    "- Compare the results with gold standard by giving the pearson correlation between them.\n",
    "\n",
    "- Questions (justify the answers):\n",
    "\n",
    "    - Which is better: words or lemmas?\n",
    "\n",
    "    - Do you think that could perform better for any pair of texts?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution\n",
    "\n",
    "Import necessary packages and declare environment valiables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/victor/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# core imports\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# scipy imports\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# constants definition\n",
    "DATA_FOLDER = Path('./trial')"
   ]
  },
  {
   "source": [
    "First functions for reading and structuring the data are declared, then the input data is read which has multiple lines containing \\[id, sentence1, sentence2\\]. The Gold standard info is also read. Then the inputText is formatted into a dict object with the following format for readability:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": <id_string>,\n",
    "    \"sent1\": <sentence_string>,\n",
    "    \"sent2\": <sentence_string>\n",
    "}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.'},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.'},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.'},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.'},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.'},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.'}]\n"
     ]
    }
   ],
   "source": [
    "def readFile(filePath):\n",
    "    \"\"\"\n",
    "    reads and returns a list of lists containing the text split by line \n",
    "    jumps and by tab characters\n",
    "    \"\"\"\n",
    "    with open(filePath, 'r') as fileHandler:\n",
    "        data = fileHandler.readlines()\n",
    "    \n",
    "    # split every line by tabs\n",
    "    data = list(map(lambda x: x.strip().split('\\t'), data))\n",
    "    return data\n",
    "\n",
    "def toDict(line):\n",
    "    \"\"\"\n",
    "    creates a dict with fields id sent1 sent2 from the values in line\n",
    "    \"\"\"\n",
    "    keys = (\"id\", \"sent1\", \"sent2\")\n",
    "    return dict(zip(keys, line))\n",
    "\n",
    "# read file data\n",
    "inputText = readFile(DATA_FOLDER / 'STS.input.txt')\n",
    "gsText = readFile(DATA_FOLDER / 'STS.gs.txt')\n",
    "\n",
    "# convert to previously defined dict structure\n",
    "inputText = list(map(toDict, inputText))\n",
    "pprint(inputText)"
   ]
  },
  {
   "source": [
    "As well as the `computeSimilarity` function similar to the function in the `S2` deliverable, a new word lemmatier function that extends the `lemmatizer.lemmatize` function from NLTK. The mentioned function will first make use of the synsets of the `nltk.corpus.wordnet` module to infer which part of speech tag to asign to the word. In order to do so, all the synsets in `nltk.corpus.wordnet` are polled to get which tag corresponds to the word according to that synset. After the types are computed, the most common is used as a kwarg in the `lemmatize.lemmatize` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizeWord(word, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    Function responsible of:\n",
    "    - detecting the part of speech type of the word\n",
    "    - if a type is detected, lemmatize\n",
    "    \"\"\"\n",
    "    types = list(map(lambda ss: ss.pos(), wn.synsets(word)))\n",
    "    if len(types) == 0:\n",
    "        return word\n",
    "    wordType = Counter(types).most_common()[0][0]\n",
    "    return lemmatizer.lemmatize(word, wordType)\n",
    "\n",
    "def computeSimilarity(sentencePair):\n",
    "    \"\"\"\n",
    "    function responsible of:\n",
    "    - tokenizing the words in the sentence\n",
    "    - converting to set\n",
    "    - computing the jaccard_distance metric\n",
    "    \"\"\"\n",
    "    sent1 = set(nltk.word_tokenize(sentencePair['sent1'], language='english'))\n",
    "    sent2 = set(nltk.word_tokenize(sentencePair['sent2'], language='english'))\n",
    "    lemmatizedSent1 = set(map(lemmatizeWord, sent1))\n",
    "    lemmatizedSent2 = set(map(lemmatizeWord, sent2))\n",
    "    return jaccard_distance(lemmatizedSent1, lemmatizedSent2)"
   ]
  },
  {
   "source": [
    "The distances are computed using both functions defined previously."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pearsonr([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], [0.69, 0.67, 0.53, 0.55, 0.77, 0.86]) = 0.490670810375692\n"
     ]
    }
   ],
   "source": [
    "testDistances = list(map(computeSimilarity, inputText))\n",
    "refDistances = [float(value)/(len(gsText)-1) for _, value in gsText]\n",
    "\n",
    "pcorr = pearsonr(refDistances, testDistances)[0]\n",
    "\n",
    "# formatting for purely demonstrative purposes\n",
    "print(f\"pearsonr({list(map(lambda x:float('%.2f' % x), refDistances))}, {list(map(lambda x:float('%.2f' % x), testDistances))}) = {pcorr}\")"
   ]
  },
  {
   "source": [
    "***\n",
    "\n",
    "## Conclusion\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The correlation metric has augmented 0.1 points compared to the correlation without using the lemmatization technique. This shows that this metric is a better reperentation of the metric in the gold standard. However, It is apparent that the metric is not yet accurate enough. As an example, we'll take the `id2` sentence as it is the only one that has an improvement (from 0.73 to 0.66 with a gs of 0.2 normalized or 1 without normalizing) from the previous approach."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"In May 2010, the troops attempted to invade Kabul.\" / \"The US army invaded Kabul on May 7th last year, 2010.\"\nsentence1 transformed words: {'troop', 'attempt', 'troops', 'attempted'}\nsentence2 transformed words: {'invade', 'invaded'}\n0.7368421052631579 -> 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "sampleSentencePair = inputText[1]\n",
    "print(f\"\\\"{sampleSentencePair['sent1']}\\\" / \\\"{sampleSentencePair['sent2']}\\\"\")\n",
    "sent1 = set(nltk.word_tokenize(sampleSentencePair['sent1'], language='english'))\n",
    "sent2 = set(nltk.word_tokenize(sampleSentencePair['sent2'], language='english'))\n",
    "lemmatizedSent1 = set(map(lemmatizeWord, sent1))\n",
    "lemmatizedSent2 = set(map(lemmatizeWord, sent2))\n",
    "print(f\"sentence1 transformed words: {lemmatizedSent1.union(sent1) - lemmatizedSent1.intersection(sent1)}\")\n",
    "print(f\"sentence2 transformed words: {lemmatizedSent2.union(sent2) - lemmatizedSent2.intersection(sent2)}\")\n",
    "print(f\"{jaccard_distance(sent1, sent2)} -> {jaccard_distance(lemmatizedSent1, lemmatizedSent2)}\")\n"
   ]
  },
  {
   "source": [
    "as it can be observed above, the word invaded in the second sentence is lemmatized to invade, because of that the similarity between the sentences increases and the distance diminishes.\n",
    "\n",
    "It can be argued that lemmas work best in most opf the situations as it eliminates the derivation differences between nouns and verbs, which should give a better similarity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "\n",
    "### End of P3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}