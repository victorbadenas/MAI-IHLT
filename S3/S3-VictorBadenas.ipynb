{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "display_name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d95a5ba215b5c0d3c5562e0f5b2730844d9eca1086482e66ee8573214ced8df7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab.3: Morphology\n",
    "## Introduction to Human Language Technologies\n",
    "### Victor Badenas Crespo\n",
    "\n",
    "***\n",
    "\n",
    "### Statement:\n",
    "\n",
    "- Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "\n",
    "- Compute their similarities by considering lemmas and Jaccard distance.\n",
    "\n",
    "- Compare the results with those in session 2 (document structure) in which words were considered.\n",
    "\n",
    "- Compare the results with gold standard by giving the pearson correlation between them.\n",
    "\n",
    "- Questions (justify the answers):\n",
    "\n",
    "    - Which is better: words or lemmas?\n",
    "\n",
    "    - Do you think that could perform better for any pair of texts?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.metrics import jaccard_distance\n",
    "from pathlib import Path\n",
    "nltk.download('punkt')\n",
    "\n",
    "DATA_FOLDER = Path('./trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.'},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.'},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.'},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.'},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.'},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.'}]\n"
     ]
    }
   ],
   "source": [
    "def readFile(filePath):\n",
    "    \"\"\"\n",
    "    reads and returns a list of lists containing the text split by line \n",
    "    jumps and by tab characters\n",
    "    \"\"\"\n",
    "    with open(filePath, 'r') as fileHandler:\n",
    "        data = fileHandler.readlines()\n",
    "    \n",
    "    # split every line by tabs\n",
    "    data = list(map(lambda x: x.strip().split('\\t'), data))\n",
    "    return data\n",
    "\n",
    "def toDict(line):\n",
    "    \"\"\"\n",
    "    creates a dict with fields id sent1 sent2 from the values in line\n",
    "    \"\"\"\n",
    "    keys = (\"id\", \"sent1\", \"sent2\")\n",
    "    return dict(zip(keys, line))\n",
    "\n",
    "# read file data\n",
    "inputText = readFile(DATA_FOLDER / 'STS.input.txt')\n",
    "gsText = readFile(DATA_FOLDER / 'STS.gs.txt')\n",
    "\n",
    "# convert to previously defined dict structure\n",
    "inputText = list(map(toDict, inputText))\n",
    "pprint(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(sentencePair):\n",
    "    \"\"\"\n",
    "    function responsible of:\n",
    "    - tokenizing the words in the sentence\n",
    "    - converting to set\n",
    "    - computing the jaccard_distance metric\n",
    "    \"\"\"\n",
    "    sent1 = set(nltk.word_tokenize(sentencePair['sent1'], language='english'))\n",
    "    sent2 = set(nltk.word_tokenize(sentencePair['sent2'], language='english'))\n",
    "    return jaccard_distance(sent1, sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pearsonr([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], [0.69, 0.74, 0.53, 0.55, 0.77, 0.86]) = 0.3962389776119232\n"
     ]
    }
   ],
   "source": [
    "testDistances = list(map(computeSimilarity, inputText))\n",
    "refDistances = [float(value)/(len(gsText)-1) for _, value in gsText]\n",
    "\n",
    "pcorr = pearsonr(refDistances, testDistances)[0]\n",
    "\n",
    "# formatting for purely demonstrative purposes\n",
    "print(f\"pearsonr({list(map(lambda x:float('%.2f' % x), refDistances))}, {list(map(lambda x:float('%.2f' % x), testDistances))}) = {pcorr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}