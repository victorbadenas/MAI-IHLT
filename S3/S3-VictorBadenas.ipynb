{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "display_name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d95a5ba215b5c0d3c5562e0f5b2730844d9eca1086482e66ee8573214ced8df7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab.3: Morphology\n",
    "## Introduction to Human Language Technologies\n",
    "### Victor Badenas Crespo\n",
    "\n",
    "***\n",
    "\n",
    "### Statement:\n",
    "\n",
    "- Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "\n",
    "- Compute their similarities by considering lemmas and Jaccard distance.\n",
    "\n",
    "- Compare the results with those in session 2 (document structure) in which words were considered.\n",
    "\n",
    "- Compare the results with gold standard by giving the pearson correlation between them.\n",
    "\n",
    "- Questions (justify the answers):\n",
    "\n",
    "    - Which is better: words or lemmas?\n",
    "\n",
    "    - Do you think that could perform better for any pair of texts?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution\n",
    "\n",
    "Import necessary packages and declare environment valiables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/victor/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# core imports\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# scipy imports\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# constants definition\n",
    "DATA_FOLDER = Path('./trial')"
   ]
  },
  {
   "source": [
    "First functions for reading and structuring the data are declared, then the input data is read which has multiple lines containing \\[id, sentence1, sentence2\\]. The Gold standard info is also read. Then the inputText is formatted into a dict object with the following format for readability:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": <id_string>,\n",
    "    \"sent1\": <sentence_string>,\n",
    "    \"sent2\": <sentence_string>\n",
    "}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.'},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.'},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.'},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.'},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.'},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.'}]\n"
     ]
    }
   ],
   "source": [
    "def readFile(filePath):\n",
    "    \"\"\"\n",
    "    reads and returns a list of lists containing the text split by line \n",
    "    jumps and by tab characters\n",
    "    \"\"\"\n",
    "    with open(filePath, 'r') as fileHandler:\n",
    "        data = fileHandler.readlines()\n",
    "    \n",
    "    # split every line by tabs\n",
    "    data = list(map(lambda x: x.strip().split('\\t'), data))\n",
    "    return data\n",
    "\n",
    "def toDict(line):\n",
    "    \"\"\"\n",
    "    creates a dict with fields id sent1 sent2 from the values in line\n",
    "    \"\"\"\n",
    "    keys = (\"id\", \"sent1\", \"sent2\")\n",
    "    return dict(zip(keys, line))\n",
    "\n",
    "# read file data\n",
    "inputText = readFile(DATA_FOLDER / 'STS.input.txt')\n",
    "gsText = readFile(DATA_FOLDER / 'STS.gs.txt')\n",
    "\n",
    "# convert to previously defined dict structure\n",
    "inputText = list(map(toDict, inputText))\n",
    "pprint(inputText)"
   ]
  },
  {
   "source": [
    "As well as the `computeSimilarity` function similar to the function in the `S2` deliverable, a new word lemmatier function that extends the `lemmatizer.lemmatize` function from NLTK. The mentioned function will first make use of the synsets of the `nltk.corpus.wordnet` module to infer which part of speech tag to asign to the word. In order to do so, all the synsets in `nltk.corpus.wordnet` are polled to get which tag corresponds to the word according to that synset. After the types are computed, the most common is used as a kwarg in the `lemmatize.lemmatize` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizeWord(word, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    Function responsible of:\n",
    "    - detecting the part of speech type of the word\n",
    "    - if a type is detected, lemmatize\n",
    "    \"\"\"\n",
    "    if word[1][0] in {'N','V'}:\n",
    "        return lemmatizer.lemmatize(word[0], pos=word[1][0].lower())\n",
    "    return word[0]\n",
    "\n",
    "def computeSimilarity(sentencePair):\n",
    "    \"\"\"\n",
    "    function responsible of:\n",
    "    - tokenizing the words in the sentence\n",
    "    - converting to set\n",
    "    - computing the jaccard_distance metric\n",
    "    \"\"\"\n",
    "    sent1 = set(nltk.word_tokenize(sentencePair['sent1'], language='english'))\n",
    "    sent2 = set(nltk.word_tokenize(sentencePair['sent2'], language='english'))\n",
    "    t_POS_sent1 = nltk.pos_tag(sent1)\n",
    "    t_POS_sent2 = nltk.pos_tag(sent2)\n",
    "    t_POS_sent1 = [(word.lower(), pos) for word, pos in t_POS_sent1]\n",
    "    t_POS_sent2 = [(word.lower(), pos) for word, pos in t_POS_sent2]\n",
    "    lemmatizedSent1 = set(map(lemmatizeWord, t_POS_sent1))\n",
    "    lemmatizedSent2 = set(map(lemmatizeWord, t_POS_sent2))\n",
    "    return jaccard_distance(lemmatizedSent1, lemmatizedSent2)"
   ]
  },
  {
   "source": [
    "The distances are computed using both functions defined previously."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pearsonr([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], [0.67, 0.59, 0.43, 0.55, 0.83, 0.86]) = 0.5790860088205632\n"
     ]
    }
   ],
   "source": [
    "testDistances = list(map(computeSimilarity, inputText))\n",
    "refDistances = [float(value)/(len(gsText)-1) for _, value in gsText]\n",
    "\n",
    "pcorr = pearsonr(refDistances, testDistances)[0]\n",
    "\n",
    "# formatting for purely demonstrative purposes\n",
    "print(f\"pearsonr({list(map(lambda x:float('%.2f' % x), refDistances))}, {list(map(lambda x:float('%.2f' % x), testDistances))}) = {pcorr}\")"
   ]
  },
  {
   "source": [
    "***\n",
    "\n",
    "## Conclusion\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The correlation metric has augmented 0.19 points compared to the correlation without using the lemmatization technique. This shows that this metric is a better reperentation of the metric in the gold standard. However, It is apparent that the metric is not yet accurate enough. As an example, we'll take the `id2` sentence:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"In May 2010, the troops attempted to invade Kabul.\" / \"The US army invaded Kabul on May 7th last year, 2010.\"\nRaw intersecion: {',', 'May', 'Kabul', '2010', '.'}\nLowercase and lemmatization intersecion: {'may', ',', '2010', '.', 'the', 'invade', 'kabul'}\nChanged words: {'may', 'May', 'Kabul', 'the', 'invade', 'kabul'}\n0.7368421052631579 -> 0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "sampleSentencePair = inputText[1]\n",
    "print(f\"\\\"{sampleSentencePair['sent1']}\\\" / \\\"{sampleSentencePair['sent2']}\\\"\")\n",
    "sent1 = set(nltk.word_tokenize(sampleSentencePair['sent1'], language='english'))\n",
    "sent2 = set(nltk.word_tokenize(sampleSentencePair['sent2'], language='english'))\n",
    "t_POS_sent1 = nltk.pos_tag(sent1)\n",
    "t_POS_sent2 = nltk.pos_tag(sent2)\n",
    "t_POS_sent1 = [(word.lower(), pos) for word, pos in t_POS_sent1]\n",
    "t_POS_sent2 = [(word.lower(), pos) for word, pos in t_POS_sent2]\n",
    "lemmatizedSent1 = set(map(lemmatizeWord, t_POS_sent1))\n",
    "lemmatizedSent2 = set(map(lemmatizeWord, t_POS_sent2))\n",
    "prevIntersection = sent1.intersection(sent2)\n",
    "newIntersection = lemmatizedSent1.intersection(lemmatizedSent2)\n",
    "print(\"Raw intersecion:\", prevIntersection)\n",
    "print(\"Lowercase and lemmatization intersecion:\", newIntersection)\n",
    "print(\"Changed words:\", prevIntersection.union(newIntersecion) - prevIntersection.intersection(newIntersection))\n",
    "print(f\"{jaccard_distance(sent1, sent2)} -> {jaccard_distance(lemmatizedSent1, lemmatizedSent2)}\")\n"
   ]
  },
  {
   "source": [
    "as it can be observed above, the words that have been changed are `may, kabul, the, invade`:\n",
    "- `may`: effect of the lowercase conversion. As they were already common without the lowercase conversion, it does not change the distance metric. \n",
    "- `kabul`: same reasoning as `may`\n",
    "- `the`: this word was present in both words with different capitalization so, the lowercase convsersion made it work better, even though it is a stopword.\n",
    "- `invade`: The main word changed with meaning. As it is changed to not contain the past extension, the distance diminishes.\n",
    "\n",
    "Because of that the similarity between the sentences increases and the distance diminishes by 0.15 points.\n",
    "\n",
    "It can be argued that lemmas work best in most of the situations as it eliminates the derivation differences between nouns and verbs, which should give a better similarity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "\n",
    "### End of P3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}