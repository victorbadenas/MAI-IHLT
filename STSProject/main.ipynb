{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from collections.abc import Iterable\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/victorbadenas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/victorbadenas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/victorbadenas/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_data\n",
    "from dimension.lexical import *\n",
    "from dimension.syntactical import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data samples: 2234, test_data samples: 3108\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_data('data/')\n",
    "print(\n",
    "    f\"train_data samples: {len(train_data)}, test_data samples: {len(test_data)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>Gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "      <td>But other sources close to the sale said Viven...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Micron has declared its first quarterly profit...</td>\n",
       "      <td>Micron's numbers also marked the first quarter...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fines are part of failed Republican effort...</td>\n",
       "      <td>Perry said he backs the Senate's efforts, incl...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "      <td>The American Anglican Council, which represent...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n",
       "      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  S1  \\\n",
       "0  But other sources close to the sale said Viven...   \n",
       "1  Micron has declared its first quarterly profit...   \n",
       "2  The fines are part of failed Republican effort...   \n",
       "3  The American Anglican Council, which represent...   \n",
       "4  The tech-loaded Nasdaq composite rose 20.96 po...   \n",
       "\n",
       "                                                  S2    Gs  \n",
       "0  But other sources close to the sale said Viven...  4.00  \n",
       "1  Micron's numbers also marked the first quarter...  3.75  \n",
       "2  Perry said he backs the Senate's efforts, incl...  2.80  \n",
       "3  The American Anglican Council, which represent...  3.40  \n",
       "4  The technology-laced Nasdaq Composite Index <....  2.40  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>Gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The problem likely will mean corrective change...</td>\n",
       "      <td>He said the problem needs to be corrected befo...</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The technology-laced Nasdaq Composite Index .I...</td>\n",
       "      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n",
       "      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEC Chairman William Donaldson said there is a...</td>\n",
       "      <td>\"I think there's a building confidence that th...</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n",
       "      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  S1  \\\n",
       "0  The problem likely will mean corrective change...   \n",
       "1  The technology-laced Nasdaq Composite Index .I...   \n",
       "2  \"It's a huge black eye,\" said publisher Arthur...   \n",
       "3  SEC Chairman William Donaldson said there is a...   \n",
       "4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n",
       "\n",
       "                                                  S2   Gs  \n",
       "0  He said the problem needs to be corrected befo...  4.4  \n",
       "1  The broad Standard & Poor's 500 Index .SPX inc...  0.8  \n",
       "2  \"It's a huge black eye,\" Arthur Sulzberger, th...  3.6  \n",
       "3  \"I think there's a building confidence that th...  3.4  \n",
       "4  In New York, Vivendi shares were 1.4 percent d...  1.4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(s1, s2):\n",
    "    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n",
    "    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n",
    "    return 1 - jaccard_distance(set(s1), set(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_similarity(s1, s2):\n",
    "    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n",
    "    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n",
    "    s1 = set(s1)\n",
    "    s2 = set(s2)\n",
    "    intersection = s1.intersection(s2)\n",
    "    return len(intersection) / min(len(s1), len(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(s1, s2):\n",
    "    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n",
    "    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n",
    "    s1 = set(s1)\n",
    "    s2 = set(s2)\n",
    "    intersection = s1.intersection(s2)\n",
    "    return len(intersection) / ((len(s1) * len(s2))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_similarity(s1, s2):\n",
    "    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n",
    "    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n",
    "    s1 = set(s1)\n",
    "    s2 = set(s2)\n",
    "    intersection = s1.intersection(s2)\n",
    "    return 2 * len(intersection) / (len(s1) + len(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Feature loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature vector builder for dataframe of sentence pairs\n",
    "\n",
    "Declaration of the function responsible for the iteration over the dataframe containing the sentence pairs (other columns shall be unused). Requires the sentences columns' to be named `\"S1\"` and `\"S2\"`.\n",
    "\n",
    "Returns a numpy array of shape `(n_sentence_pairs, n_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df: pd.DataFrame):\n",
    "    assert \"S1\" in df.columns, \"S1 not in dataframe\"\n",
    "    assert \"S2\" in df.columns, \"S2 not in dataframe\"\n",
    "\n",
    "    features = [None] * len(df)   #preallocated for memory efficiency\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sentence1, sentence2 = row['S1'], row['S2']\n",
    "\n",
    "        # Get all words\n",
    "        tokenized_1, tokenized_2 = get_tokenized_sentences(\n",
    "            sentence1, sentence2, return_unique_words=False)\n",
    "        tokenized_lc_1, tokenized_lc_2 = get_tokenized_sentences_lowercase(\n",
    "            tokenized_1, tokenized_2, return_unique_words=False)\n",
    "\n",
    "        # Get words without stopwords\n",
    "        no_stopwords_1, no_stopwords_2 = filter_stopwords(\n",
    "            tokenized_1, tokenized_2, return_unique_words=False)\n",
    "        no_stopwords_lc_1, no_stopwords_lc_2 = filter_stopwords(\n",
    "            tokenized_lc_1, tokenized_lc_2, return_unique_words=False)\n",
    "\n",
    "        # Lemmas\n",
    "        lemmatized_1, lemmatized_2 = get_lemmas(tokenized_1,\n",
    "                                                tokenized_2,\n",
    "                                                return_unique_words=False)\n",
    "        lemmatized_lc_1, lemmatized_lc_2 = get_lemmas(\n",
    "            tokenized_lc_1, tokenized_lc_2, return_unique_words=False)\n",
    "\n",
    "        # Name entities\n",
    "        sentence_ne_1, sentence_ne_2 = get_named_entities(\n",
    "            tokenized_1, tokenized_2)\n",
    "        \n",
    "        #lemmas cleaned from stopwords\n",
    "        stopwords_and_lemmas1, stopwords_and_lemmas2 = get_lemmas(\n",
    "            no_stopwords_1, no_stopwords_2, return_unique_words=False)\n",
    "\n",
    "        stopwords_and_lemmas_lc_1, stopwords_and_lemmas_lc_2 = get_lemmas(\n",
    "            no_stopwords_lc_1, no_stopwords_lc_2, return_unique_words=False)\n",
    "        \n",
    "        # Name entities without stopwords in lowercase\n",
    "        ne_no_stopwords_1, ne_no_stopwords_2 = filter_stopwords(\n",
    "            sentence_ne_1, sentence_ne_2, return_unique_words=False, filter_and_return_in_lowercase=True)\n",
    "        \n",
    "        # Name entities without stopwords in lowercase and lemmas\n",
    "        ne_no_stopwords_lemmas_1, ne_no_stopwords_lemmas_2 = get_lemmas( ne_no_stopwords_1, ne_no_stopwords_2, \n",
    "                                                                        return_unique_words=False)\n",
    "\n",
    "        # Bigrams\n",
    "        bigrams_1, bigrams_2 = get_ngrams(no_stopwords_1, no_stopwords_2, n=2)\n",
    "        trigrams_1, trigrams_2 = get_ngrams(no_stopwords_1, no_stopwords_2, n=3)\n",
    "        \n",
    "        # Bigrams trigrams with sentence tokenizer \n",
    "        bigrams_sent_1, bigrams_sent_2 = get_ngrams_with_sent_tokenize(sentence1, sentence2, n=2)\n",
    "        trigrams_sent_1, trigrams_sent_2 = get_ngrams_with_sent_tokenize(sentence1, sentence2, n=3)\n",
    "        \n",
    "        # Lesk\n",
    "        lesk_1, lesk_2 = get_lesk_sentences(tokenized_1, tokenized_2)\n",
    "        lesk_lc_1, lesk_lc_2 = get_lesk_sentences(tokenized_lc_1, tokenized_lc_2)\n",
    "\n",
    "\n",
    "        # Stemmer\n",
    "        stemmed_1, stemmed_2 = get_stemmed_sentences(sentence1, sentence2)\n",
    "        \n",
    "        \n",
    "        # Synset\n",
    "        average_path = get_synset_similarity(tokenized_1, tokenized_2, \"path\")\n",
    "        average_lch = get_synset_similarity(tokenized_1, tokenized_2, \"lch\")\n",
    "        average_wup = get_synset_similarity(tokenized_1, tokenized_2, \"wup\")\n",
    "        average_lin = get_synset_similarity(tokenized_1, tokenized_2, \"lin\")\n",
    "        \n",
    "        average_lc_path = get_synset_similarity(tokenized_lc_1, tokenized_lc_2, \"path\")\n",
    "        average_lc_lch = get_synset_similarity(tokenized_lc_1, tokenized_lc_2, \"lch\")\n",
    "        average_lc_wup = get_synset_similarity(tokenized_lc_1, tokenized_lc_2, \"wup\")\n",
    "        average_lc_lin = get_synset_similarity(tokenized_lc_1, tokenized_lc_2, \"lin\")\n",
    "        \n",
    "        \n",
    "        # ALL Features\n",
    "        features[index] = [\n",
    "            jaccard_similarity(tokenized_1, tokenized_2),\n",
    "            # jaccard_similarity(tokenized_lc_1, tokenized_lc_2),\n",
    "            # jaccard_similarity(no_stopwords_1, no_stopwords_2),\n",
    "            jaccard_similarity(no_stopwords_lc_1, no_stopwords_lc_2),\n",
    "            # jaccard_similarity(lemmatized_1, lemmatized_2),\n",
    "            # jaccard_similarity(lemmatized_lc_1, lemmatized_lc_2),\n",
    "            jaccard_similarity(sentence_ne_1, sentence_ne_2),\n",
    "            jaccard_similarity(stopwords_and_lemmas1, stopwords_and_lemmas2),\n",
    "            # jaccard_similarity(stopwords_and_lemmas_lc_1, stopwords_and_lemmas_lc_2),\n",
    "            # jaccard_similarity(bigrams_1, bigrams_2),\n",
    "            jaccard_similarity(trigrams_1, trigrams_2),\n",
    "            # jaccard_similarity(bigrams_sent_1, bigrams_sent_2),\n",
    "            jaccard_similarity(trigrams_sent_1, trigrams_sent_2),\n",
    "            jaccard_similarity(lesk_1, lesk_2),\n",
    "            jaccard_similarity(lesk_lc_1, lesk_lc_2),\n",
    "            jaccard_similarity(stemmed_1, stemmed_2),\n",
    "            dice_similarity(tokenized_1, tokenized_2),\n",
    "            # dice_similarity(tokenized_lc_1, tokenized_lc_2),\n",
    "            dice_similarity(no_stopwords_1, no_stopwords_2),\n",
    "            dice_similarity(no_stopwords_lc_1, no_stopwords_lc_2),\n",
    "            # dice_similarity(lemmatized_1, lemmatized_2),\n",
    "            # dice_similarity(lemmatized_lc_1, lemmatized_lc_2),\n",
    "            # dice_similarity(sentence_ne_1, sentence_ne_2),\n",
    "            # dice_similarity(stopwords_and_lemmas1, stopwords_and_lemmas2),\n",
    "            dice_similarity(stopwords_and_lemmas_lc_1, stopwords_and_lemmas_lc_2),\n",
    "            dice_similarity(bigrams_1, bigrams_2),\n",
    "            # dice_similarity(trigrams_1, trigrams_2),\n",
    "            # dice_similarity(bigrams_sent_1, bigrams_sent_2),\n",
    "            # dice_similarity(trigrams_sent_1, trigrams_sent_2),\n",
    "            # dice_similarity(lesk_1, lesk_2),\n",
    "            # dice_similarity(lesk_lc_1, lesk_lc_2),\n",
    "            # dice_similarity(stemmed_1, stemmed_2),\n",
    "            # average_path,\n",
    "            # average_lch,\n",
    "            # average_wup,\n",
    "            average_lin,\n",
    "            # average_lc_path,\n",
    "            average_lc_lch,\n",
    "            average_lc_wup,\n",
    "            average_lc_lin\n",
    "        ]\n",
    "        # BEST Features selection \n",
    "        \"\"\"features[index] = [\n",
    "            jaccard_similarity(tokenized_1, tokenized_2),\n",
    "            jaccard_similarity(no_stopwords_lc_1, no_stopwords_lc_2),\n",
    "            jaccard_similarity(sentence_ne_1, sentence_ne_2),\n",
    "            jaccard_similarity(stopwords_and_lemmas1, stopwords_and_lemmas2),\n",
    "            jaccard_similarity(trigrams_1, trigrams_2),\n",
    "            jaccard_similarity(trigrams_sent_1, trigrams_sent_2),\n",
    "            jaccard_similarity(lesk_1, lesk_2),\n",
    "            jaccard_similarity(lesk_lc_1, lesk_lc_2),\n",
    "            jaccard_similarity(stemmed_1, stemmed_2),\n",
    "            dice_similarity(tokenized_1, tokenized_2),\n",
    "            dice_similarity(no_stopwords_1, no_stopwords_2),\n",
    "            dice_similarity(no_stopwords_lc_1, no_stopwords_lc_2),\n",
    "            dice_similarity(stopwords_and_lemmas_lc_1, stopwords_and_lemmas_lc_2),\n",
    "            dice_similarity(bigrams_1, bigrams_2),\n",
    "            average_lin,\n",
    "            average_lc_lch,\n",
    "            average_lc_wup,\n",
    "            average_lc_lin\n",
    "        ]\"\"\"\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0- (4.0)\n",
      "But other sources close to the sale said Vivendi was keeping the door open to further bids and hoped to see bidders interested in individual assets team up.\n",
      "But other sources close to the sale said Vivendi was keeping the door open for further bids in the next day or two.\n",
      "-----------------------------\n",
      "['But', 'other.a.01', 'reservoir.n.04', 'near.r.01', 'to', 'the', 'sale.n.05', 'state.v.01', 'Vivendi', 'be.v.12', 'retain.v.02', 'the', 'doorway.n.01', 'overt.a.01', 'to', 'further', 'bid.n.04', 'and', 'hope.v.01', 'to', 'visualize.v.01', 'bidder.n.02', 'interested.a.01', 'in', 'individual.a.01', 'assets.n.01', 'team.v.01', 'up.r.03', '.']\n",
      "['But', 'other.a.01', 'reservoir.n.04', 'near.r.01', 'to', 'the', 'sale.n.05', 'pronounce.v.01', 'Vivendi', 'be.v.12', 'retain.v.02', 'the', 'doorway.n.01', 'receptive.a.02', 'for', 'further', 'command.n.01', 'in', 'the', 'next', 'sidereal_day.n.01', 'or', 'two', '.']\n",
      "********************************************************\n",
      "-1- (3.75)\n",
      "Micron has declared its first quarterly profit for three years.\n",
      "Micron's numbers also marked the first quarterly profit in three years for the DRAM manufacturer.\n",
      "-----------------------------\n",
      "['micron.n.01', 'hold.v.03', 'declare.v.08', 'its', 'first.a.06', 'quarterly.a.01', 'profit.n.02', 'for', 'three', 'year.n.03', '.']\n",
      "['micron.n.01', \"'s\", 'phone_number.n.01', 'besides.r.02', 'set.v.04', 'the', 'first.a.06', 'quarterly.a.01', 'net_income.n.01', 'in', 'three', 'year.n.03', 'for', 'the', 'dram.n.03', 'manufacturer.n.01', '.']\n",
      "********************************************************\n",
      "-2- (2.8)\n",
      "The fines are part of failed Republican efforts to force or entice the Democrats to return.\n",
      "Perry said he backs the Senate's efforts, including the fines, to force the Democrats to return.\n",
      "-----------------------------\n",
      "['The', 'fine.n.01', 'be.v.01', 'function.n.03', 'of', 'failed', 'republican.n.01', 'effort.n.02', 'to', 'impel.v.01', 'or', 'entice.v.01', 'the', 'democrat.n.01', 'to', 'return.v.05', '.']\n",
      "['perry.n.03', 'order.v.01', 'he', 'second.v.01', 'the', 'united_states_senate.n.01', \"'s\", 'attempt.n.01', ',', 'admit.v.03', 'the', 'fine.n.01', ',', 'to', 'impel.v.01', 'the', 'democrat.n.01', 'to', 'return.v.14', '.']\n",
      "********************************************************\n",
      "-3- (3.4)\n",
      "The American Anglican Council, which represents Episcopalian conservatives, said it will seek authorization to create a separate group.\n",
      "The American Anglican Council, which represents Episcopalian conservatives, said it will seek authorization to create a separate province in North America because of last week's actions.\n",
      "-----------------------------\n",
      "['The', 'american.a.02', 'anglican.n.01', 'council.n.03', ',', 'which', 'represent.v.12', 'episcopal.a.01', 'conservative.n.01', ',', 'suppose.v.01', 'it', 'will', 'seek.v.04', 'mandate.n.01', 'to', 'produce.v.02', 'a', 'separate.a.01', 'group.n.03', '.']\n",
      "['The', 'american.a.01', 'anglican.n.01', 'council.n.03', ',', 'which', 'represent.v.04', 'episcopal.a.01', 'conservative.n.02', ',', 'pronounce.v.01', 'it', 'will', 'search.v.01', 'mandate.n.01', 'to', 'produce.v.02', 'a', 'separate.a.01', 'state.n.01', 'in', 'north.n.07', 'united_states.n.01', 'because', 'of', 'last.a.02', 'workweek.n.01', \"'s\", 'natural_process.n.01', '.']\n",
      "********************************************************\n",
      "-4- (2.4)\n",
      "The tech-loaded Nasdaq composite rose 20.96 points to 1595.91, ending at its highest level for 12 months.\n",
      "The technology-laced Nasdaq Composite Index <.IXIC> climbed 19.11 points, or 1.2 percent, to 1,615.02.\n",
      "-----------------------------\n",
      "['The', 'tech-loaded', 'national_association_of_securities_dealers_automated_quotations.n.01', 'composite.n.02', 'surface.v.01', '20.96', 'period.n.07', 'to', '1595.91', ',', 'end.v.04', 'at', 'its', 'high.a.02', 'horizontal_surface.n.01', 'for', '12', 'month.n.02', '.']\n",
      "['The', 'technology-laced', 'national_association_of_securities_dealers_automated_quotations.n.01', 'composite.n.02', 'index.n.01', '<', '.IXIC', '>', 'rise.v.02', '19.11', 'period.n.07', ',', 'or', '1.2', 'percentage.n.01', ',', 'to', '1,615.02', '.']\n",
      "********************************************************\n"
     ]
    }
   ],
   "source": [
    "# TEST CELL\n",
    "def show_sentences_with_applications(train_head, results):\n",
    "    for index, row in train_head.iterrows():\n",
    "        sentence1, sentence2 = row['S1'], row['S2']\n",
    "        gs = row['Gs']\n",
    "        print(f\"-{index}- ({gs})\")\n",
    "        print(sentence1)\n",
    "        print(sentence2)\n",
    "        if(results is not None):\n",
    "            for result in results:\n",
    "                print(\"-----------------------------\")\n",
    "                print(result[index][0])\n",
    "                print(result[index][1])\n",
    "        print(\"********************************************************\")\n",
    "        \n",
    "def apply_function(data, method_to_apply, params = None):\n",
    "    results = []\n",
    "    for sentence1, sentence2 in data:\n",
    "        if params is None:\n",
    "            result1, result2 = method_to_apply(sentence1, sentence2)\n",
    "        elif len(params) == 1:\n",
    "            result1, result2 = method_to_apply(sentence1, sentence2, params[0])\n",
    "        elif len(params) == 2:\n",
    "            result1, result2 = method_to_apply(sentence1, sentence2, params[0], params[1])  \n",
    "        results.append((result1, result2))\n",
    "    return results\n",
    "\n",
    "\n",
    "train_head = train_data.head()\n",
    "sentence_list = [(row['S1'], row['S2']) for index, row in train_head.iterrows()]\n",
    "\n",
    "results = []\n",
    "tokens = apply_function(sentence_list, get_tokenized_sentences)\n",
    "lesk = apply_function(tokens, get_lesk_sentences)\n",
    "results.append(lesk)\n",
    "show_sentences_with_applications(train_head, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Bonnie', 'White', 'lies', 'over', 'the', 'ocean', ',', 'in', 'Picadilli', 'Circus', 'at', '3:00pm', '.']\n",
      "['My', 'Bonnie', 'lied', 'over', 'the', 'sea', '!', 'Over', 'the', 'sea', '...']\n",
      "['My', 'Bonnie White', 'lies', 'ocean', 'Picadilli Circus', '3:00pm']\n",
      "['My', 'Bonnie', 'lied', 'sea', 'Over', 'sea', '...']\n",
      "0.19868326118326116\n",
      "0.5725251800393162\n",
      "0.34814814814814815\n",
      "0.3526340967251205\n"
     ]
    }
   ],
   "source": [
    "# TEST cell don't delete it =D\n",
    "\n",
    "first = \"My Bonnie White lies over the ocean, in Picadilli Circus at 3:00pm.\"\n",
    "second = \"My Bonnie lied over the sea! Over the sea...\"\n",
    "\n",
    "tokenized_1, tokenized_2 = get_tokenized_sentences(first,\n",
    "                                                   second,\n",
    "                                                   return_unique_words=False)\n",
    "no_stopwords_1, no_stopwords_2 = filter_stopwords(tokenized_1, tokenized_2, return_unique_words=False)\n",
    "sentence_ne_1, sentence_ne_2 = get_named_entities(no_stopwords_1, no_stopwords_2)\n",
    "\n",
    "print(tokenized_1)\n",
    "print(tokenized_2)\n",
    "print(sentence_ne_1)\n",
    "print(sentence_ne_2)\n",
    "\n",
    "average_sim = get_synset_similarity(tokenized_1, tokenized_2, \"path\")\n",
    "print(average_sim)\n",
    "average_sim = get_synset_similarity(tokenized_1, tokenized_2, \"lch\")\n",
    "print(average_sim)\n",
    "average_sim = get_synset_similarity(tokenized_1, tokenized_2, \"wup\")\n",
    "print(average_sim)\n",
    "average_sim = get_synset_similarity(tokenized_1, tokenized_2, \"lin\")\n",
    "print(average_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0- (4.0)\n",
      "But other sources close to the sale said Vivendi was keeping the door open to further bids and hoped to see bidders interested in individual assets team up.\n",
      "But other sources close to the sale said Vivendi was keeping the door open for further bids in the next day or two.\n",
      "-----------------------------\n",
      "['But', 'other', 'sources', 'close', 'to', 'the', 'sale', 'said', 'Vivendi', 'was', 'keeping', 'the', 'door', 'open', 'to', 'further', 'bids', 'and', 'hoped', 'to', 'see', 'bidders', 'interested', 'in', 'individual', 'assets', 'team', 'up', '.']\n",
      "['But', 'other', 'sources', 'close', 'to', 'the', 'sale', 'said', 'Vivendi', 'was', 'keeping', 'the', 'door', 'open', 'for', 'further', 'bids', 'in', 'the', 'next', 'day', 'or', 'two', '.']\n",
      "********************************************************\n",
      "-1- (3.75)\n",
      "Micron has declared its first quarterly profit for three years.\n",
      "Micron's numbers also marked the first quarterly profit in three years for the DRAM manufacturer.\n",
      "-----------------------------\n",
      "['Micron', 'has', 'declared', 'its', 'first', 'quarterly', 'profit', 'for', 'three', 'years', '.']\n",
      "['Micron', \"'s\", 'numbers', 'also', 'marked', 'the', 'first', 'quarterly', 'profit', 'in', 'three', 'years', 'for', 'the', 'DRAM', 'manufacturer', '.']\n",
      "********************************************************\n",
      "-2- (2.8)\n",
      "The fines are part of failed Republican efforts to force or entice the Democrats to return.\n",
      "Perry said he backs the Senate's efforts, including the fines, to force the Democrats to return.\n",
      "-----------------------------\n",
      "['The', 'fines', 'are', 'part', 'of', 'failed', 'Republican', 'efforts', 'to', 'force', 'or', 'entice', 'the', 'Democrats', 'to', 'return', '.']\n",
      "['Perry', 'said', 'he', 'backs', 'the', 'Senate', \"'s\", 'efforts', ',', 'including', 'the', 'fines', ',', 'to', 'force', 'the', 'Democrats', 'to', 'return', '.']\n",
      "********************************************************\n",
      "-3- (3.4)\n",
      "The American Anglican Council, which represents Episcopalian conservatives, said it will seek authorization to create a separate group.\n",
      "The American Anglican Council, which represents Episcopalian conservatives, said it will seek authorization to create a separate province in North America because of last week's actions.\n",
      "-----------------------------\n",
      "['The', 'American', 'Anglican', 'Council', ',', 'which', 'represents', 'Episcopalian', 'conservatives', ',', 'said', 'it', 'will', 'seek', 'authorization', 'to', 'create', 'a', 'separate', 'group', '.']\n",
      "['The', 'American', 'Anglican', 'Council', ',', 'which', 'represents', 'Episcopalian', 'conservatives', ',', 'said', 'it', 'will', 'seek', 'authorization', 'to', 'create', 'a', 'separate', 'province', 'in', 'North', 'America', 'because', 'of', 'last', 'week', \"'s\", 'actions', '.']\n",
      "********************************************************\n",
      "-4- (2.4)\n",
      "The tech-loaded Nasdaq composite rose 20.96 points to 1595.91, ending at its highest level for 12 months.\n",
      "The technology-laced Nasdaq Composite Index <.IXIC> climbed 19.11 points, or 1.2 percent, to 1,615.02.\n",
      "-----------------------------\n",
      "['The', 'tech-loaded', 'Nasdaq', 'composite', 'rose', '20.96', 'points', 'to', '1595.91', ',', 'ending', 'at', 'its', 'highest', 'level', 'for', '12', 'months', '.']\n",
      "['The', 'technology-laced', 'Nasdaq', 'Composite', 'Index', '<', '.IXIC', '>', 'climbed', '19.11', 'points', ',', 'or', '1.2', 'percent', ',', 'to', '1,615.02', '.']\n",
      "********************************************************\n"
     ]
    }
   ],
   "source": [
    "# TEST CELL\n",
    "def show_sentences_with_applications(train_head, results):\n",
    "    for index, row in train_head.iterrows():\n",
    "        sentence1, sentence2 = row['S1'], row['S2']\n",
    "        gs = row['Gs']\n",
    "        print(f\"-{index}- ({gs})\")\n",
    "        print(sentence1)\n",
    "        print(sentence2)\n",
    "        if(results is not None):\n",
    "            for result in results:\n",
    "                print(\"-----------------------------\")\n",
    "                print(result[index][0])\n",
    "                print(result[index][1])\n",
    "        print(\"********************************************************\")\n",
    "        \n",
    "def apply_function(data, method_to_apply, params = None):\n",
    "    results = []\n",
    "    for sentence1, sentence2 in data:\n",
    "        if params is None:\n",
    "            result1, result2 = method_to_apply(sentence1, sentence2)\n",
    "        elif len(params) == 1:\n",
    "            result1, result2 = method_to_apply(sentence1, sentence2, params[0])\n",
    "        elif len(params) == 2:\n",
    "            result1, result2 = method_to_apply(sentence1, sentence2, params[0], params[1])  \n",
    "        results.append((result1, result2))\n",
    "    return results\n",
    "\n",
    "\n",
    "train_head = train_data.head()\n",
    "sentence_list = [(row['S1'], row['S2']) for index, row in train_head.iterrows()]\n",
    "\n",
    "results = []\n",
    "tokens = apply_function(sentence_list, get_tokenized_sentences)\n",
    "results.append(tokens)\n",
    "\n",
    "show_sentences_with_applications(train_head, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Bonnie', 'White', 'lies', 'over', 'the', 'ocean', ',', 'in', 'Picadilli', 'Circus', 'at', '3:00pm', '.']\n",
      "['My', 'Bonnie', 'lied', 'over', 'the', 'sea', '!', 'Over', 'the', 'sea', '...']\n",
      "['My', 'Bonnie White', 'lies', 'ocean', 'Picadilli Circus', '3:00pm']\n",
      "['My', 'Bonnie', 'lied', 'sea', 'Over', 'sea', '...']\n"
     ]
    }
   ],
   "source": [
    "# TEST cell don't delete it =D\n",
    "\n",
    "first = \"My Bonnie White lies over the ocean, in Picadilli Circus at 3:00pm.\"\n",
    "second = \"My Bonnie lied over the sea! Over the sea...\"\n",
    "\n",
    "tokenized_1, tokenized_2 = get_tokenized_sentences(first,\n",
    "                                                   second,\n",
    "                                                   return_unique_words=False)\n",
    "no_stopwords_1, no_stopwords_2 = filter_stopwords(tokenized_1, tokenized_2, return_unique_words=False)\n",
    "sentence_ne_1, sentence_ne_2 = get_named_entities(no_stopwords_1, no_stopwords_2)\n",
    "\n",
    "print(tokenized_1)\n",
    "print(tokenized_2)\n",
    "print(sentence_ne_1)\n",
    "print(sentence_ne_2)\n",
    "#TEST cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction\n",
    "\n",
    "Using the function declared above, the features are extracted from the `train_data` dataframe. Also the Gold Standard is extracted from its column in the dataframe. The shapes for both numpy vectors are displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.shape: (2234, 18)\n",
      "train_gs.shape: (2234,)\n"
     ]
    }
   ],
   "source": [
    "train_features = get_features(train_data)\n",
    "train_gs = train_data['Gs'].to_numpy()\n",
    "print(f\"train_features.shape: {train_features.shape}\")\n",
    "print(f\"train_gs.shape: {train_gs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.shape: (3108, 18)\n",
      "train_gs.shape: (3108,)\n"
     ]
    }
   ],
   "source": [
    "test_features = get_features(test_data)\n",
    "test_gs = test_data['Gs'].to_numpy()\n",
    "print(f\"train_features.shape: {test_features.shape}\")\n",
    "print(f\"train_gs.shape: {test_gs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature scaling\n",
    "\n",
    "features are scaled using sklearns StandardScaler, where the mean is substracted for each feature and it's divided by the variance of the feature to obtain a unified feature space with zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_features)\n",
    "train_features_scaled = scaler.transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split definition for GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5342, 18) (5342,)\n"
     ]
    }
   ],
   "source": [
    "all_data = np.concatenate([train_features_scaled, test_features_scaled])\n",
    "all_labels = np.concatenate([train_gs, test_gs])\n",
    "test_fold = np.array([-1]*train_features_scaled.shape[0] + [0]*test_features_scaled.shape[0])\n",
    "print(all_data.shape, test_fold.shape)\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SVR Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 840 candidates, totalling 840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 684 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 840 out of 840 | elapsed:   27.1s finished\n"
     ]
    }
   ],
   "source": [
    "pearson_scorer = make_scorer(lambda y, y_hat: pearsonr(y, y_hat)[0])\n",
    "\n",
    "gammas = np.logspace(-6, -1, 6)\n",
    "Cs = np.array([0.5, 1, 2, 4, 8, 10, 15, 20, 50, 100, 200, 375, 500, 1000])\n",
    "epsilons = np.linspace(0.1, 1, 10)\n",
    "param = dict(gamma=gammas, C=Cs, epsilon=epsilons)\n",
    "\n",
    "svr = SVR(kernel='rbf', tol=1)\n",
    "gssvr = GridSearchCV(svr,\n",
    "                     param,\n",
    "                     cv=ps,\n",
    "                     scoring=pearson_scorer,\n",
    "                     n_jobs=-1,\n",
    "                     verbose=1)\n",
    "gssvr = gssvr.fit(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = gssvr.best_params_\n",
    "best_model = SVR(kernel='rbf', tol=1, **best_parameters)\n",
    "train_predictions = best_model.fit(train_features_scaled, train_gs).predict(train_features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = best_model.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_correlation = pearsonr(train_predictions, train_gs)[0]\n",
    "test_correlation = pearsonr(test_predictions, test_gs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pearsonr:  0.8612974649511776\n",
      "Test pearsonr:  0.7456997951303734\n",
      "The best value of gamma: 0.1\n",
      "The best value of C: 1.0\n",
      "The best value of epsilon: 0.4\n"
     ]
    }
   ],
   "source": [
    "print('Train pearsonr: ', train_correlation)\n",
    "print('Test pearsonr: ', test_correlation)\n",
    "print('The best value of gamma:', gssvr.best_estimator_.gamma)\n",
    "print('The best value of C:', gssvr.best_estimator_.C)\n",
    "print('The best value of epsilon:', gssvr.best_estimator_.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Do not delete ;)\n",
    "\n",
    "## Recorded Results\n",
    "|SVR|jaccard|overlap|cosine|dice|jaccard+dice|no_stop_lc_dice|no_stop_lemmas_dice|no_stop_lemmas_lc_dice|forward_search_12_feats|mutual_info_sel_13_feats|pca_12_feats|forward_search_pca_6_feats|fs_pca_12_features|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|train correlation|0.6402|0.6233|0.3732|0.6397|0.6379|0.6017|0.6465|0.649|0.6544|0.6364|0.6454|0.6669|0.6485|\n",
    "|test_correlation|0.6208|0.6198|0.1056|0.6483|0.6087|0.5905|0.6634|0.652|0.6843|0.6428|0.665|0.6755|0.6848|\n",
    "|C|10.0|500.0|100.0|375.0|1000.0|2.0|100.0|500.0|15.0|20.0|500.0|20.0|2.0|\n",
    "|gamma|0.01|0.01|0.1|0.0001|0.001|0.1|0.01|1e-5|0.01|0.001|0.001|0.01|0.01|\n",
    "|epsilon|0.9|1.0|1.0|0.9|0.2|0.1|0.9|0.1|0.8|0.3|1.0|0.8|0.4|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
