{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d95a5ba215b5c0d3c5562e0f5b2730844d9eca1086482e66ee8573214ced8df7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab 7: Word Sequences\n",
    "## Introduction to Human Language Technologies\n",
    "### Victor Badenas Crespo\n",
    "\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Statement:\n",
    "\n",
    "- Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "- Compute their similarities by considering the following approach:\n",
    "    - words plus NEs and Jaccard coefficient ex: word_and_NEs=\\['John Smith', 'is', 'working'\\]\n",
    "- Show the results.\n",
    "- Do you think it could be relevant to use NEs to compute the similarity between two sentences? Justify the answer.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package words to /Users/victor/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/victor/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /Users/victor/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package conll2000 to\n[nltk_data]     /Users/victor/nltk_data...\n[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# core imports\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# scipy imports\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "# constants definition\n",
    "DATA_FOLDER = Path('./trial')"
   ]
  },
  {
   "source": [
    "First functions for reading and structuring the data are declared, then the input data is read which has multiple lines containing \\[id, sentence1, sentence2\\]. The Gold standard info is also read. Then the inputText is formatted into a dict object with the following format for readability:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": <id_string>,\n",
    "    \"sent1\": <sentence_string>,\n",
    "    \"sent2\": <sentence_string>\n",
    "}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.'},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.'},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.'},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.'},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.'},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.'}]\n"
     ]
    }
   ],
   "source": [
    "def readFile(filePath):\n",
    "    \"\"\"\n",
    "    reads and returns a list of lists containing the text split by line \n",
    "    jumps and by tab characters\n",
    "    \"\"\"\n",
    "    with open(filePath, 'r') as fileHandler:\n",
    "        data = fileHandler.readlines()\n",
    "    \n",
    "    # split every line by tabs\n",
    "    data = list(map(lambda x: x.strip().split('\\t'), data))\n",
    "    return data\n",
    "\n",
    "def toDict(line):\n",
    "    \"\"\"\n",
    "    creates a dict with fields id sent1 sent2 from the values in line\n",
    "    \"\"\"\n",
    "    keys = (\"id\", \"sent1\", \"sent2\")\n",
    "    return dict(zip(keys, line))\n",
    "\n",
    "# read file data\n",
    "inputText = readFile(DATA_FOLDER / 'STS.input.txt')\n",
    "gsText = readFile(DATA_FOLDER / 'STS.gs.txt')\n",
    "\n",
    "# convert to previously defined dict structure\n",
    "inputText = list(map(toDict, inputText))\n",
    "pprint(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in inputText:\n",
    "    id, sent1, sent2 = sentence[\"id\"], sentence[\"sent1\"], sentence[\"sent2\"]\n",
    "    sent1 = nltk.word_tokenize(sent1)\n",
    "    sent2 = nltk.word_tokenize(sent2)\n",
    "    t_POS_sent1 = nltk.pos_tag(sent1)\n",
    "    t_POS_sent2 = nltk.pos_tag(sent2)\n",
    "    sentence[\"ne_chunk1\"] = nltk.ne_chunk(t_POS_sent1)\n",
    "    sentence[\"ne_chunk2\"] = nltk.ne_chunk(t_POS_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'id': 'id1',\n 'ne_chunk1': Tree('S', [('The', 'DT'), ('bird', 'NN'), ('is', 'VBZ'), ('bathing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('sink', 'NN'), ('.', '.')]),\n 'ne_chunk2': Tree('S', [Tree('GPE', [('Birdie', 'NNP')]), ('is', 'VBZ'), ('washing', 'VBG'), ('itself', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('water', 'NN'), ('basin', 'NN'), ('.', '.')]),\n 'sent1': 'The bird is bathing in the sink.',\n 'sent2': 'Birdie is washing itself in the water basin.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(inputText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(sentenceDict):\n",
    "    context1 = set(sentenceDict[\"ne_chunk1\"])\n",
    "    context2 = set(sentenceDict[\"ne_chunk2\"])\n",
    "    return jaccard_distance(context1, context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tree'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-de7dd1c61dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestDistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputeSimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrefDistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgsText\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefDistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3bbb4e57dfbb>\u001b[0m in \u001b[0;36mcomputeSimilarity\u001b[0;34m(sentenceDict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomputeSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcontext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ne_chunk1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcontext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ne_chunk2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjaccard_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Tree'"
     ]
    }
   ],
   "source": [
    "testDistances = list(map(computeSimilarity, inputText))\n",
    "refDistances = [float(value) for _, value in gsText]\n",
    "\n",
    "pcorr = pearsonr(refDistances, testDistances)[0]\n",
    "\n",
    "# formatting for purely demonstrative purposes\n",
    "print(f\"pearsonr({list(map(lambda x:float('%.2f' % x), refDistances))}, {list(map(lambda x:float('%.2f' % x), testDistances))}) = {pcorr}\")"
   ]
  },
  {
   "source": [
    "***\n",
    "\n",
    "## Conlusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "***\n",
    "\n",
    "### End of P4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}