{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d95a5ba215b5c0d3c5562e0f5b2730844d9eca1086482e66ee8573214ced8df7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab.5: Lexical semantics\n",
    "## Introduction to Human Language Technologies\n",
    "### Victor Badenas Crespo\n",
    "\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Statement\n",
    "\n",
    "- Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "- Apply Leskâ€™s algorithm to the words in the sentences.\n",
    "- Compute their similarities by considering senses and Jaccard coefficient.\n",
    "- Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n",
    "- Compare the results with gold standard by giving the pearson correlation between them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "\n",
    "## Solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# core imports\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# scipy imports\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# constants definition\n",
    "DATA_FOLDER = Path('./trial')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/victor/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/victor/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ]
  },
  {
   "source": [
    "First functions for reading and structuring the data are declared, then the input data is read which has multiple lines containing \\[id, sentence1, sentence2\\]. The Gold standard info is also read. Then the inputText is formatted into a dict object with the following format for readability:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": <id_string>,\n",
    "    \"sent1\": <sentence_string>,\n",
    "    \"sent2\": <sentence_string>\n",
    "}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.'},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.'},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.'},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.'},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.'},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.'}]\n"
     ]
    }
   ],
   "source": [
    "def readFile(filePath):\n",
    "    \"\"\"\n",
    "    reads and returns a list of lists containing the text split by line \n",
    "    jumps and by tab characters\n",
    "    \"\"\"\n",
    "    with open(filePath, 'r') as fileHandler:\n",
    "        data = fileHandler.readlines()\n",
    "    \n",
    "    # split every line by tabs\n",
    "    data = list(map(lambda x: x.strip().split('\\t'), data))\n",
    "    return data\n",
    "\n",
    "def toDict(line):\n",
    "    \"\"\"\n",
    "    creates a dict with fields id sent1 sent2 from the values in line\n",
    "    \"\"\"\n",
    "    keys = (\"id\", \"sent1\", \"sent2\")\n",
    "    return dict(zip(keys, line))\n",
    "\n",
    "# read file data\n",
    "inputText = readFile(DATA_FOLDER / 'STS.input.txt')\n",
    "gsText = readFile(DATA_FOLDER / 'STS.gs.txt')\n",
    "\n",
    "# convert to previously defined dict structure\n",
    "inputText = list(map(toDict, inputText))\n",
    "pprint(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in inputText:\n",
    "    id, sent1, sent2 = sentence[\"id\"], sentence[\"sent1\"], sentence[\"sent2\"]\n",
    "    sent1 = nltk.word_tokenize(sent1)\n",
    "    sent2 = nltk.word_tokenize(sent2)\n",
    "    t_POS_sent1 = nltk.pos_tag(sent1)\n",
    "    t_POS_sent2 = nltk.pos_tag(sent2)\n",
    "    synsets1 = list(map(lambda word: lesk(sent1, word[0], pos=word[1][0].lower()), t_POS_sent1))\n",
    "    synsets2 = list(map(lambda word: lesk(sent2, word[0], pos=word[1][0].lower()), t_POS_sent2))\n",
    "    synsets1 = list(filter(lambda i: i is not None, synsets1))\n",
    "    synsets2 = list(filter(lambda i: i is not None, synsets2))\n",
    "\n",
    "    sentence[\"synsets1\"] = synsets1\n",
    "    sentence[\"synsets2\"] = synsets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.',\n  'synsets1': [Synset('bird.n.02'),\n               Synset('be.v.12'),\n               Synset('bathe.v.01'),\n               Synset('sinkhole.n.01')],\n  'synsets2': [Synset('shuttlecock.n.01'),\n               Synset('be.v.12'),\n               Synset('wash.v.09'),\n               Synset('body_of_water.n.01'),\n               Synset('washbasin.n.01')]},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.',\n  'synsets1': [Synset('whitethorn.n.01'),\n               Synset('troop.n.02'),\n               Synset('undertake.v.01'),\n               Synset('invade.v.01'),\n               Synset('kabul.n.01')],\n  'synsets2': [Synset('uranium.n.01'),\n               Synset('united_states_army.n.01'),\n               Synset('invade.v.03'),\n               Synset('kabul.n.01'),\n               Synset('whitethorn.n.01'),\n               Synset('year.n.02')]},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.',\n  'synsets1': [Synset('whoremaster.n.01'),\n               Synset('suppose.v.01'),\n               Synset('embody.v.02'),\n               Synset('view.v.02'),\n               Synset('witness.n.05'),\n               Synset('not.r.01'),\n               Synset('defendant.n.01')],\n  'synsets2': [Synset('embody.v.02'),\n               Synset('not.r.01'),\n               Synset('defendant.n.01'),\n               Synset('anymore.r.01'),\n               Synset('whoremaster.n.01'),\n               Synset('suppose.v.01')]},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.',\n  'synsets1': [Synset('fly.v.12'), Synset('group.n.02')],\n  'synsets2': [Synset('fly.v.10'), Synset('together.r.04')]},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.',\n  'synsets1': [Synset('woman.n.02'),\n               Synset('be.v.01'),\n               Synset('play.v.35'),\n               Synset('violin.n.01')],\n  'synsets2': [Synset('lady.n.03'),\n               Synset('love.v.02'),\n               Synset('heed.v.01'),\n               Synset('guitar.n.01')]},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.',\n  'synsets1': [Synset('toilet.n.01'),\n               Synset('plump.v.04'),\n               Synset('knight.n.02'),\n               Synset('back.r.02'),\n               Synset('ride.v.13'),\n               Synset('dawn.n.01'),\n               Synset('group.n.02'),\n               Synset('friend.n.05')],\n  'synsets2': [Synset('sunrise.n.03'),\n               Synset('dawn.n.03'),\n               Synset('be.v.12'),\n               Synset('view.n.07'),\n               Synset('take.v.34'),\n               Synset('awaken.v.01'),\n               Synset('up.r.05'),\n               Synset('early_on.r.01'),\n               Synset('enough.r.01')]}]\n"
     ]
    }
   ],
   "source": [
    "pprint(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(sentenceDict):\n",
    "    context1 = set(sentenceDict[\"synsets1\"])\n",
    "    context2 = set(sentenceDict[\"synsets2\"])\n",
    "    return jaccard_distance(context1, context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDistances = list(map(computeSimilarity, inputText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pearsonr([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], [0.88, 0.78, 0.38, 1.0, 1.0, 1.0]) = 0.4195840591519665\n"
     ]
    }
   ],
   "source": [
    "refDistances = [float(value)/(len(gsText)-1) for _, value in gsText]\n",
    "\n",
    "pcorr = pearsonr(refDistances, testDistances)[0]\n",
    "\n",
    "# formatting for purely demonstrative purposes\n",
    "print(f\"pearsonr({list(map(lambda x:float('%.2f' % x), refDistances))}, {list(map(lambda x:float('%.2f' % x), testDistances))}) = {pcorr}\")"
   ]
  },
  {
   "source": [
    "*** \n",
    "\n",
    "## Conlusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The correlation value is almost the same than in the previous practicums. The higher value in the correlation overall may suggest that the computation of the synsets and the distance being computed by the jaccard_distance of the synsets may detect more similarities between the sentences, but maybe too many. The distance values show that it overestimates the distance of the sentences. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "\n",
    "### End of P4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}