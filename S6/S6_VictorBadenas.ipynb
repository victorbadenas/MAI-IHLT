{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "display_name": "Python 3.7.9 64-bit ('IHLT3.7': conda)",
   "metadata": {
    "interpreter": {
     "hash": "589da564c3c34ee97637a5e347a7928039a22cef64d507784db9da12a80e8881"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab.5: Lexical semantics\n",
    "## Introduction to Human Language Technologies\n",
    "### Victor Badenas Crespo\n",
    "\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Statement\n",
    "\n",
    "- Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "- Apply Leskâ€™s algorithm to the words in the sentences.\n",
    "- Compute their similarities by considering senses and Jaccard coefficient.\n",
    "- Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n",
    "- Compare the results with gold standard by giving the pearson correlation between them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "\n",
    "## Solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# core imports\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# scipy imports\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# constants definition\n",
    "DATA_FOLDER = Path('./trial')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ]
  },
  {
   "source": [
    "First functions for reading and structuring the data are declared, then the input data is read which has multiple lines containing \\[id, sentence1, sentence2\\]. The Gold standard info is also read. Then the inputText is formatted into a dict object with the following format for readability:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": <id_string>,\n",
    "    \"sent1\": <sentence_string>,\n",
    "    \"sent2\": <sentence_string>\n",
    "}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'id': 'id1',\n  'sent1': 'The bird is bathing in the sink.',\n  'sent2': 'Birdie is washing itself in the water basin.'},\n {'id': 'id2',\n  'sent1': 'In May 2010, the troops attempted to invade Kabul.',\n  'sent2': 'The US army invaded Kabul on May 7th last year, 2010.'},\n {'id': 'id3',\n  'sent1': 'John said he is considered a witness but not a suspect.',\n  'sent2': '\"He is not a suspect anymore.\" John said.'},\n {'id': 'id4',\n  'sent1': 'They flew out of the nest in groups.',\n  'sent2': 'They flew into the nest together.'},\n {'id': 'id5',\n  'sent1': 'The woman is playing the violin.',\n  'sent2': 'The young lady enjoys listening to the guitar.'},\n {'id': 'id6',\n  'sent1': 'John went horse back riding at dawn with a whole group of friends.',\n  'sent2': 'Sunrise at dawn is a magnificent view to take in if you wake up '\n           'early enough for it.'}]\n"
     ]
    }
   ],
   "source": [
    "def readFile(filePath):\n",
    "    \"\"\"\n",
    "    reads and returns a list of lists containing the text split by line \n",
    "    jumps and by tab characters\n",
    "    \"\"\"\n",
    "    with open(filePath, 'r') as fileHandler:\n",
    "        data = fileHandler.readlines()\n",
    "    \n",
    "    # split every line by tabs\n",
    "    data = list(map(lambda x: x.strip().split('\\t'), data))\n",
    "    return data\n",
    "\n",
    "def toDict(line):\n",
    "    \"\"\"\n",
    "    creates a dict with fields id sent1 sent2 from the values in line\n",
    "    \"\"\"\n",
    "    keys = (\"id\", \"sent1\", \"sent2\")\n",
    "    return dict(zip(keys, line))\n",
    "\n",
    "# read file data\n",
    "inputText = readFile(DATA_FOLDER / 'STS.input.txt')\n",
    "gsText = readFile(DATA_FOLDER / 'STS.gs.txt')\n",
    "\n",
    "# convert to previously defined dict structure\n",
    "inputText = list(map(toDict, inputText))\n",
    "pprint(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('The', 'DT'), ('bird', 'NN'), ('is', 'VBZ'), ('bathing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('sink', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "lesk() missing 1 required positional argument: 'ambiguous_word'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a51a83c613ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mt_POS_sent2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_POS_sent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msynsets1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_POS_sent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msynsets2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_POS_sent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msynsets1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynsets1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a51a83c613ac>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mt_POS_sent2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_POS_sent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msynsets1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_POS_sent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msynsets2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_POS_sent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msynsets1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynsets1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: lesk() missing 1 required positional argument: 'ambiguous_word'"
     ]
    }
   ],
   "source": [
    "for sentence in inputText:\n",
    "    id, sent1, sent2 = sentence[\"id\"], sentence[\"sent1\"], sentence[\"sent2\"]\n",
    "    sent1 = nltk.word_tokenize(sent1)\n",
    "    sent2 = nltk.word_tokenize(sent2)\n",
    "    t_POS_sent1 = nltk.pos_tag(sent1)\n",
    "    t_POS_sent2 = nltk.pos_tag(sent2)\n",
    "    print(t_POS_sent1)\n",
    "    synsets1 = list(map(lambda word: lesk(sent1, word[0], pos=word[1][0].lower()), t_POS_sent1))\n",
    "    synsets2 = list(map(lambda word: lesk(sent1, word[0], pos=word[1][0].lower()), t_POS_sent2))\n",
    "    synsets1 = list(filter(lambda i: i is not None, synsets1))\n",
    "    synsets2 = list(filter(lambda i: i is not None, synsets2))\n",
    "\n",
    "    sentence[\"synsets1\"] = synsets1\n",
    "    sentence[\"synsets2\"] = synsets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(sentenceDict):\n",
    "    context1 = set(sentenceDict[\"synsets1\"])\n",
    "    context2 = set(sentenceDict[\"synsets2\"])\n",
    "    return jaccard_distance(context1, context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDistances = list(map(computeSimilarity, inputText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refDistances = [float(value)/(len(gsText)-1) for _, value in gsText]\n",
    "\n",
    "pcorr = pearsonr(refDistances, testDistances)[0]\n",
    "\n",
    "# formatting for purely demonstrative purposes\n",
    "print(f\"pearsonr({list(map(lambda x:float('%.2f' % x), refDistances))}, {list(map(lambda x:float('%.2f' % x), testDistances))}) = {pcorr}\")"
   ]
  },
  {
   "source": [
    "*** \n",
    "\n",
    "## Conlusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**placeholder**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "\n",
    "### End of P4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}